{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LeNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mindspore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfYueMjO7h_5",
        "outputId": "d7609a7b-a0de-47d7-aecd-8c5df066af3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mindspore in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.13.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (21.3)\n",
            "Requirement already satisfied: scipy>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from mindspore) (1.7.3)\n",
            "Requirement already satisfied: psutil>=5.6.1 in /usr/local/lib/python3.7/dist-packages (from mindspore) (5.9.0)\n",
            "Requirement already satisfied: asttokens>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mindspore) (2.0.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from asttokens>=2.0.0->mindspore) (1.15.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->mindspore) (3.0.8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "L0kEvsEH7t7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "import requests\n",
        "import numpy as np\n",
        "\n",
        "from mindspore import context\n",
        "import mindspore.dataset as ds\n",
        "import mindspore.dataset.transforms.c_transforms as C\n",
        "import mindspore.dataset.vision.c_transforms as CV\n",
        "from mindspore.dataset.vision import Inter\n",
        "from mindspore import dtype as mstype\n",
        "import mindspore.nn as nn\n",
        "from mindspore.common.initializer import Normal\n",
        "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig\n",
        "from mindspore.nn import Accuracy\n",
        "from mindspore.train.callback import LossMonitor\n",
        "from mindspore import Model\n",
        "from mindspore import load_checkpoint, load_param_into_net\n",
        "from mindspore import Tensor\n",
        "from mindspore.train.callback import SummaryCollector\n",
        "from mindspore.profiler import Profiler\n",
        "from mindspore.ops import TensorSummary\n",
        "from mindspore.ops import ImageSummary"
      ],
      "metadata": {
        "id": "JTwSieYj7yTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "Db9CpincUuvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(dataset_url, path):\n",
        "    filename = dataset_url.split(\"/\")[-1]\n",
        "    save_path = os.path.join(path, filename)\n",
        "    if os.path.exists(save_path):\n",
        "        return\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    res = requests.get(dataset_url, stream=True, verify=False)\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        for chunk in res.iter_content(chunk_size=512):\n",
        "            if chunk:\n",
        "                f.write(chunk)"
      ],
      "metadata": {
        "id": "ytXFeT0-UwmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset(data_path, batch_size=32, repeat_size=1,\n",
        "                   num_parallel_workers=1):\n",
        "    # Define the dataset.\n",
        "    mnist_ds = ds.MnistDataset(data_path)\n",
        "    resize_height, resize_width = 32, 32\n",
        "    rescale = 1.0 / 255.0\n",
        "    shift = 0.0\n",
        "    rescale_nml = 1 / 0.3081\n",
        "    shift_nml = -1 * 0.1307 / 0.3081\n",
        "\n",
        "    # Define the mapping to be operated.\n",
        "    resize_op = CV.Resize((resize_height, resize_width), interpolation=Inter.LINEAR)\n",
        "    rescale_nml_op = CV.Rescale(rescale_nml, shift_nml)\n",
        "    rescale_op = CV.Rescale(rescale, shift)\n",
        "    hwc2chw_op = CV.HWC2CHW()\n",
        "    type_cast_op = C.TypeCast(mstype.int32)\n",
        "\n",
        "    # Use the map function to apply data operations to the dataset.\n",
        "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=num_parallel_workers)\n",
        "    mnist_ds = mnist_ds.map(operations=[resize_op, rescale_op, rescale_nml_op, hwc2chw_op], input_columns=\"image\", num_parallel_workers=num_parallel_workers)\n",
        "\n",
        "\n",
        "    # Perform shuffle, batch and repeat operations.\n",
        "    buffer_size = 10000\n",
        "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
        "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
        "    mnist_ds = mnist_ds.repeat(count=repeat_size)\n",
        "\n",
        "    return mnist_ds"
      ],
      "metadata": {
        "id": "X5DGJEWdUzV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet5(nn.Cell):\n",
        "    \"\"\"\n",
        "    Lenet network structure\n",
        "    \"\"\"\n",
        "    def __init__(self, num_class=10, num_channel=1):\n",
        "        super(LeNet5, self).__init__()\n",
        "        # Define the required operation.\n",
        "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
        "        self.fc1 = nn.Dense(16 * 5 * 5, 120, weight_init=Normal(0.02))\n",
        "        self.fc2 = nn.Dense(120, 84, weight_init=Normal(0.02))\n",
        "        self.fc3 = nn.Dense(84, num_class, weight_init=Normal(0.02))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.tensor_summary = TensorSummary()\n",
        "        self.image_summary = ImageSummary()\n",
        "\n",
        "    def construct(self, x):\n",
        "        # Use the defined operation to construct a forward network.\n",
        "        self.image_summary(\"image\", x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool2d(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        self.tensor_summary(\"tensor\", x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "g0LuHuhPU4Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_net(model, epoch_size, data_path, repeat_size, ckpoint_cb, sink_mode):\n",
        "    \"\"\"Define a training method.\"\"\"\n",
        "    # Load the training dataset.\n",
        "    ds_train = create_dataset(os.path.join(data_path, \"train\"), 32, repeat_size)\n",
        "    summary_collector = SummaryCollector(summary_dir='./summary_dir', collect_freq=1)\n",
        "    model.train(epoch_size, ds_train, callbacks=[ckpoint_cb, LossMonitor(125), summary_collector], dataset_sink_mode=sink_mode)\n",
        "\n",
        "def test_net(model, data_path):\n",
        "    \"\"\"Define a validation method.\"\"\"\n",
        "    ds_eval = create_dataset(os.path.join(data_path, \"test\"))\n",
        "    acc = model.eval(ds_eval, dataset_sink_mode=False)\n",
        "    print(\"{}\".format(acc))"
      ],
      "metadata": {
        "id": "3KJ_3eIOU-LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main part"
      ],
      "metadata": {
        "id": "xjEcG0-R74AY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16J9dTjB7dNT"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='MindSpore LeNet Example')\n",
        "parser.add_argument('--device_target', type=str, default=\"CPU\", choices=['Ascend', 'GPU', 'CPU'])\n",
        "\n",
        "args = parser.parse_known_args()[0]\n",
        "context.set_context(mode=context.GRAPH_MODE, device_target=args.device_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"datasets/MNIST_Data/train\"\n",
        "test_path = \"datasets/MNIST_Data/test\"\n",
        "\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-labels-idx1-ubyte\", train_path)\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/train-images-idx3-ubyte\", train_path)\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-labels-idx1-ubyte\", test_path)\n",
        "download_dataset(\"https://mindspore-website.obs.myhuaweicloud.com/notebook/datasets/mnist/t10k-images-idx3-ubyte\", test_path)"
      ],
      "metadata": {
        "id": "nhvKzRcz7rST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the network.\n",
        "net = LeNet5()"
      ],
      "metadata": {
        "id": "kmOurGu18I8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function.\n",
        "net_loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')"
      ],
      "metadata": {
        "id": "DMPAshPW8I_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer.\n",
        "net_opt = nn.Momentum(net.trainable_params(), learning_rate=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "7t5yaHjV8PFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set model saving parameters.\n",
        "config_ck = CheckpointConfig(save_checkpoint_steps=1875, keep_checkpoint_max=10)\n",
        "# Use model saving parameters.\n",
        "ckpoint = ModelCheckpoint(prefix=\"checkpoint_lenet\", config=config_ck)"
      ],
      "metadata": {
        "id": "yVS1OFi38PHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profiler = Profiler(output_path='./profiler_data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWfrZsJEXZ8K",
        "outputId": "46139ecc-296c-4c77-dd26-e91703f2336f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING] ME(1730:140333846534016,MainProcess):2022-05-11-14:24:53.529.775 [mindspore/profiler/profiling.py:1088] For 'Profiler', fail to get RANK_ID from environment, use 0 instead.\n",
            "[WARNING] ME(1730:140333846534016,MainProcess):2022-05-11-14:24:53.534.002 [mindspore/profiler/profiling.py:1120] The target dir already exists. There may be some old profiling data, and they will be rewritten in the end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_epoch = 1\n",
        "mnist_path = \"./datasets/MNIST_Data\"\n",
        "dataset_size = 1\n",
        "model = Model(net, net_loss, net_opt, metrics={\"Accuracy\": Accuracy()})\n",
        "\n",
        "train_net(model, train_epoch, mnist_path, dataset_size, ckpoint, False)\n",
        "\n",
        "profiler.analyse()\n",
        "\n",
        "test_net(model, mnist_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNmSDX88UHKU",
        "outputId": "ed2581d8-f69e-4dbd-ce8d-336ef40c4609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 step: 125, loss is 2.311739206314087\n",
            "epoch: 1 step: 250, loss is 2.299069881439209\n",
            "epoch: 1 step: 375, loss is 2.3176259994506836\n",
            "epoch: 1 step: 500, loss is 2.293073892593384\n",
            "epoch: 1 step: 625, loss is 2.308850049972534\n",
            "epoch: 1 step: 750, loss is 2.284219264984131\n",
            "epoch: 1 step: 875, loss is 2.296015501022339\n",
            "epoch: 1 step: 1000, loss is 2.263505458831787\n",
            "epoch: 1 step: 1125, loss is 0.35911011695861816\n",
            "epoch: 1 step: 1250, loss is 0.2936016023159027\n",
            "epoch: 1 step: 1375, loss is 0.21174485981464386\n",
            "epoch: 1 step: 1500, loss is 0.13035358488559723\n",
            "epoch: 1 step: 1625, loss is 0.02578505501151085\n",
            "epoch: 1 step: 1750, loss is 0.127903014421463\n",
            "epoch: 1 step: 1875, loss is 0.10249046981334686\n",
            "{'Accuracy': 0.9657451923076923}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model for testing.\n",
        "param_dict = load_checkpoint(\"checkpoint_lenet-1_1875.ckpt\")\n",
        "# Load parameters to the network.\n",
        "load_param_into_net(net, param_dict)"
      ],
      "metadata": {
        "id": "N8B8Lofm8PKE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c6c8be-1d3f-4108-fdae-c89a4cf7cbd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a test dataset. If batch_size is set to 1, an image is obtained.\n",
        "ds_test = create_dataset(os.path.join(mnist_path, \"test\"), batch_size=1).create_dict_iterator()\n",
        "data = next(ds_test)\n",
        "\n",
        "# `images` indicates the test image, and `labels` indicates the actual classification of the test image.\n",
        "images = data[\"image\"].asnumpy()\n",
        "labels = data[\"label\"].asnumpy()\n",
        "\n",
        "# Use the model.predict function to predict the classification of the image.\n",
        "output = model.predict(Tensor(data['image']))\n",
        "predicted = np.argmax(output.asnumpy(), axis=1)\n",
        "\n",
        "# Output the predicted classification and the actual classification.\n",
        "print(f'Predicted: \"{predicted[0]}\", Actual: \"{labels[0]}\"')"
      ],
      "metadata": {
        "id": "JPLcYLJS8PNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78b4538-9331-4277-8731-5bee2a151b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: \"0\", Actual: \"0\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBRdxocfoMwZ"
      },
      "source": [
        "# Export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WwmAGozEUVWo",
        "outputId": "095620a8-18ab-40df-c3b2-c43241a31d71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"/content/profiler_data'.zip\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import shutil\n",
        "shutil.make_archive(\"summary_dir\", \"zip\", '/content/summary_dir')\n",
        "shutil.make_archive(\"profiler_data\", \"zip\", '/content/profiler_data')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aTk6DpHEVcRe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}